{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# import [module]\n",
    "# importlib.reload([module])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def show(x):\n",
    "    plt.imshow(x,'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import custom.models\n",
    "reload(custom.models)\n",
    "from custom.models import seg_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.16) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from custom.models import conv_SVM, seg_net\n",
    "from custom.utils import get_weight_c\n",
    "from custom.datasets import nuclei_dataset, nuclei_data\n",
    "from custom.loss import weighted_bce_loss_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/notebook/data/yujie/cell_segmentation/cell_detection/custom/datasets.py:141: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  weights[labels==l] = total_count * 1. / nl / l_count\n"
     ]
    }
   ],
   "source": [
    "dat = nuclei_data(path='../data/stage1_train/')\n",
    "# dat1 = nuclei_data(path='../data/stage1_train/', label=1)\n",
    "# dat2 = nuclei_data(path='../data/stage1_train/', label=2)\n",
    "\n",
    "train_loader = DataLoader(nuclei_dataset(dat, mode='train'),batch_size=16, shuffle=True)\n",
    "full_loader = DataLoader(nuclei_dataset(dat, mode='full'),batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(nuclei_dataset(dat, mode='eval'), batch_size=8)\n",
    "\n",
    "# train_loader1 = DataLoader(nuclei_dataset(dat1),batch_size=8, shuffle=True)\n",
    "# test_loader1 = DataLoader(nuclei_dataset(dat1, train=False), batch_size=8)\n",
    "\n",
    "# train_loader2 = DataLoader(nuclei_dataset(dat2),batch_size=16, shuffle=True)\n",
    "# test_loader2 = DataLoader(nuclei_dataset(dat2, train=False), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbI2DwczWOBjMbI2DwczWOBjMbM3/AcnBOYO+4xTBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ad94b4cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im, mask, dist, _ =dat[43]\n",
    "show(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seg_net()\n",
    "# model.load_state_dict(torch.load('./models/segnet_1'))\n",
    "# pretrained_filter = torch.load('./models/segnet_1')\n",
    "# model = conv_SVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seg_net(\n",
       "  (conv_block_1): conv357_block(\n",
       "    (conv3): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv7): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (conv_block_2): conv357_block(\n",
       "    (conv3): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(192, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv7): Conv2d(192, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (conv_block_3): conv357_block(\n",
       "    (conv3): Conv2d(96, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(96, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv7): Conv2d(96, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (conv_block_4): conv357_block(\n",
       "    (conv3): Conv2d(48, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv5): Conv2d(48, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv7): Conv2d(48, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True)\n",
       "  )\n",
       "  (conv5): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_func = weighted_bce_loss_with_logits\n",
    "# loss_func = lambda x, y, z: F.hinge_embedding_loss(x, (y-.5)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "to_cuda_var = lambda x: Variable(x.float()).cuda()\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (im, mask, dist, wt) in enumerate(train_loader):\n",
    "        weight = get_weight_c(mask, factor01=(.5, .5)) + 10. * np.exp(-np.power(dist.numpy(), 2) / 25.)\n",
    "        weight = weight * wt.numpy().reshape((-1, 1, 1, 1))\n",
    "        weight = torch.from_numpy(weight)\n",
    "        input, target, weight = to_cuda_var(im), to_cuda_var(mask), to_cuda_var(weight)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_func(output, target, weight)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            n = 8\n",
    "            comparison = torch.cat([input[:n], output[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                     './results/train_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "to_cuda_var = lambda x: Variable(x.float()).cuda()\n",
    "from torch.nn.functional import threshold\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i, (im, mask, dist, _) in enumerate(test_loader):\n",
    "        weight = get_weight_c(mask, factor01=(.5, .5)) + 10. * np.exp(-np.power(dist.numpy(), 2) / 25.)\n",
    "        weight = torch.from_numpy(weight)\n",
    "        input, target, weight = to_cuda_var(im), to_cuda_var(mask), to_cuda_var(weight)\n",
    "        output = model(input)\n",
    "        test_loss += loss_func(output, target, weight).data[0]\n",
    "        label = threshold(-threshold(output, .5, 0, inplace=True), -.5, 1, inplace=True)\n",
    "        if i == 0:\n",
    "            n = 8\n",
    "            comparison = torch.cat([input[:n], label[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                     './results/test_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.0429\n",
      "====> Test set loss: 3.0186\n",
      "====> Epoch: 2 Average loss: 0.0428\n",
      "====> Test set loss: 2.9988\n",
      "====> Epoch: 3 Average loss: 0.0424\n",
      "====> Test set loss: 2.2721\n",
      "====> Epoch: 4 Average loss: 0.0402\n",
      "====> Test set loss: 3.5360\n",
      "====> Epoch: 5 Average loss: 0.0409\n",
      "====> Test set loss: 3.2760\n",
      "====> Epoch: 6 Average loss: 0.0404\n",
      "====> Test set loss: 2.7950\n",
      "====> Epoch: 7 Average loss: 0.0461\n",
      "====> Test set loss: 2.8140\n",
      "====> Epoch: 8 Average loss: 0.0391\n",
      "====> Test set loss: 2.6991\n",
      "====> Epoch: 9 Average loss: 0.0374\n",
      "====> Test set loss: 2.0996\n",
      "====> Epoch: 10 Average loss: 0.0378\n",
      "====> Test set loss: 2.6406\n",
      "====> Epoch: 11 Average loss: 0.0359\n",
      "====> Test set loss: 2.6296\n",
      "====> Epoch: 12 Average loss: 0.0374\n",
      "====> Test set loss: 2.5880\n",
      "====> Epoch: 13 Average loss: 0.0358\n",
      "====> Test set loss: 2.5064\n",
      "====> Epoch: 14 Average loss: 0.0356\n",
      "====> Test set loss: 1.9194\n",
      "====> Epoch: 15 Average loss: 0.0341\n",
      "====> Test set loss: 2.5104\n",
      "====> Epoch: 16 Average loss: 0.0334\n",
      "====> Test set loss: 1.9086\n",
      "====> Epoch: 17 Average loss: 0.0348\n",
      "====> Test set loss: 1.9289\n",
      "====> Epoch: 18 Average loss: 0.0335\n",
      "====> Test set loss: 2.3994\n",
      "====> Epoch: 19 Average loss: 0.0342\n",
      "====> Test set loss: 2.3787\n",
      "====> Epoch: 20 Average loss: 0.0326\n",
      "====> Test set loss: 2.3562\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/segnet_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
